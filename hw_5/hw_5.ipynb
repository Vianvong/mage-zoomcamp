{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1a05976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0a8e84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/03 20:49:53 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdcbd808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://dtc.europe-west1-b.c.dtc-de-412516.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8f79342d10>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b14b3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema generate\n",
    "# csv_file = \"/home/vladimir/data-engineering-zoomcamp/05-batch/code/data/raw/fhv/2019/01/fhv_tripdata_2019_01.csv.gz\"\n",
    "\n",
    "# pandas_df = pd.read_csv(csv_file)\n",
    "\n",
    "# spark_schema = StructType([\n",
    "#     StructField(name, StringType() if dtype == \"object\" else IntegerType(), True)\n",
    "#     for name, dtype in pandas_df.dtypes.items()  # Вместо iteritems() используем items()\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62344910",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True), \n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True), \n",
    "    types.StructField('dropOff_datetime', types.TimestampType(), True), \n",
    "    types.StructField('PUlocationID', types.IntegerType(), True), \n",
    "    types.StructField('DOlocationID', types.IntegerType(), True), \n",
    "    types.StructField('SR_Flag', types.IntegerType(), True), \n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61d579d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2019/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2019/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2019/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2019/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2019/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2019/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2019/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2019/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2019/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2019/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2019/11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2019/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "year = 2019\n",
    "\n",
    "for month in range(1, 13):\n",
    "    print(f'processing data for {year}/{month}')\n",
    "\n",
    "    input_path = f'/home/vladimir/data-engineering-zoomcamp/05-batch/code/data/raw/fhv/{year}/{month:02d}/'\n",
    "    output_path = f'/home/vladimir/data-engineering-zoomcamp/05-batch/code/data/pq/{year}/{month:02d}/'\n",
    "\n",
    "    df_fhv = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .schema(fhv_schema) \\\n",
    "        .csv(input_path)\n",
    "\n",
    "    df_fhv \\\n",
    "        .repartition(6) \\\n",
    "        .write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d182bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average size of Parquet files (MB): 12.07019297281901\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_parquet_files(directory):\n",
    "    parquet_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".parquet\"):\n",
    "                parquet_files.append(os.path.join(root, file))\n",
    "    return parquet_files\n",
    "\n",
    "parquet_root_dir = \"/home/vladimir/data-engineering-zoomcamp/05-batch/code/data/pq/fhv/2019\"\n",
    "\n",
    "parquet_files = get_parquet_files(parquet_root_dir)\n",
    "\n",
    "total_size = 0\n",
    "for file in parquet_files:\n",
    "    size_mb = os.path.getsize(file) / (1024 * 1024)  # Преобразовать размер в мегабайты\n",
    "    total_size += size_mb\n",
    "\n",
    "average_size = total_size / len(parquet_files)\n",
    "print(\"Average size of Parquet files (MB):\", average_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e62f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 3\n",
    "\n",
    "spark.read.parquet(\"/home/vladimir/data-engineering-zoomcamp/05-batch/code/data/pq/fhv/2019/10\") \\\n",
    "    .createOrReplaceTempView(\"data_10\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select count(1)\n",
    "from data_10\n",
    "where pickup_datetime >= '2019-10-15 00:00:00'\n",
    "  and pickup_datetime < '2019-10-16 00:00:00'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "80179e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|max(trip_time)                         |\n",
      "+---------------------------------------+\n",
      "|INTERVAL '26298 00:30:00' DAY TO SECOND|\n",
      "+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# question 4 \n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select max(trip_time)\n",
    "from (\n",
    "    select dropOff_datetime - pickup_datetime as trip_time\n",
    "    from data_10\n",
    ")\n",
    "\"\"\").show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8e5ebaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5c999031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                Zone|cnt|\n",
      "+--------------------+---+\n",
      "|         Jamaica Bay|  1|\n",
      "|Governor's Island...|  2|\n",
      "| Green-Wood Cemetery|  5|\n",
      "|       Broad Channel|  8|\n",
      "|     Highbridge Park| 14|\n",
      "|        Battery Park| 15|\n",
      "|Saint Michaels Ce...| 23|\n",
      "|Breezy Point/Fort...| 25|\n",
      "|Marine Park/Floyd...| 26|\n",
      "|        Astoria Park| 29|\n",
      "|    Inwood Hill Park| 39|\n",
      "|       Willets Point| 47|\n",
      "|Forest Park/Highl...| 53|\n",
      "|  Brooklyn Navy Yard| 57|\n",
      "|        Crotona Park| 62|\n",
      "|        Country Club| 77|\n",
      "|     Freshkills Park| 89|\n",
      "|       Prospect Park| 98|\n",
      "|     Columbia Street|105|\n",
      "|  South Williamsburg|110|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# question 6\n",
    "\n",
    "spark.read.csv(\"taxi_zone_lookup.csv\", header=True, inferSchema=True).createOrReplaceTempView(\"zones\")\n",
    "spark.read.parquet(\"/home/vladimir/data-engineering-zoomcamp/05-batch/code/data/pq/fhv/2019/10\") \\\n",
    "    .createOrReplaceTempView(\"data_10\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select zones.Zone, count(1) as cnt\n",
    "from data_10\n",
    " join zones\n",
    "   on zones.LocationID = data_10.PUlocationID\n",
    " group by zones.Zone\n",
    " order by cnt\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0df8d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
